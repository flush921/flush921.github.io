# 决策树
## 重要概念
**信息熵与条件信息熵:**
(在李航老师书中, 也名为经验熵与经验条件熵, 名字源于该两种熵是由数据估计所得到的)

![Image](https://github.com/user-attachments/assets/16d81add-e397-47bd-91b0-5f0e2cdb73a8)

**信息增益:**
表示得知特征X的信息而使得类Y的信息的不确定性减少的程度

![Image](https://github.com/user-attachments/assets/ea6a18bf-62e0-4e05-ace7-12ac526c791e)

**信息增益比:**
信息增益存在以下问题: 偏向于选择取值较多的特征的问题
eg: 我们的样本有一个属性叫 *序号*，每一个样本都具有一个单独的序号，因此使用序号划分后，每个子结点只有一个样本，熵为0。这样的话信息增益最大，算法就会以此属性作为最优划分属性。
这显然与我们的意愿不同。因此引申出了增益比的思想。

![Image](https://github.com/user-attachments/assets/01a38d93-4fa9-43b6-be34-cc6f1a85dec2)

## 决策树的生成
### ID3算法
ID3算法的**核心**是在决策树各个结点上应用 *信息增益* 准则选择特征，递归地构建决策树。
具体方法是:从根结点(rootnode)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点再对子结点递归地调用以上方法，构建决策树;直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一棵决策树。ID3相当于用极大似然法进行概率模型的选择。

![Image](https://github.com/user-attachments/assets/0260428e-632f-4834-ae43-16928b8964ee)
![Image](https://github.com/user-attachments/assets/9016c2b4-e331-467d-8491-8dfe3b1e4bb1)

### C4.5算法
相较于ID3, 只是将准则换为 *信息增益比* 。

## CART算法
以上两种算法都不涉及剪枝，易涉及到过拟合、泛化能力差的问题。并且都仅作分类算法。

CART 算法由以下**两步**组成:
(1)决策树生成:基于训练数据集生成决策树，生成的决策树要尽量大;
(2)决策树剪枝:用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

CART既可以分类也可以回归

### CART回归树生成
我们先看一下决策树回归的流程，与分类问题类似。
![Image](https://i-blog.csdnimg.cn/blog_migrate/23ab3de4c979715786731cb4db8705ef.png)
算法：
1.开始算法，将样本集划分到根节点。
2.寻找最优划分点。该划分点是指某属性（j）的某个值（s）。
3.使用最优划分点(j,s)将数据划分到子节点，将属性值小于s的划分到一个节点，属性值大于s的到另一个节点。该节点的输出值为节点均值。
4.判断是否满足终止条件，如果满足则结束；否则循环划分。终止条件有很多，比如子节点最小样本数、决策树最大深度等等
5.结束，形成回归树。

可以看出，决策树回归的重点就在两个地方：
重点1：寻找最优切分点；
重点2：将样本切分后，子节点的输出值怎么算。

重点1：
回归决策树是通过最小二乘法来寻找最优切分点（j,s）的。定义了如下目标函数：
 ![Image](https://i-blog.csdnimg.cn/blog_migrate/281ef700db98bc50259226457a041e26.png)
算法会遍历每个属性的每个值(j,s)，就假如到属性j和值s的时候，会将样本集划分为2个子集（x<=s和x>=s),我们这里称这两个子集为R1和R2。
首先，我们要先寻找最优的C1使得R1的误差平方和最小，C2使得R2的误差平方和最小。这个数学上很容易证明，当C1和C2分别为子集R1和R2的y的均值的时候成立。上式也可以写成：
![Image](https://i-blog.csdnimg.cn/blog_migrate/e8352dc8c3927d09a30e289688b4e92a.png)
这样对于每一个（j,s），我们都会根据上式得到一个数值。然后我们取能使得上式取值最小的（j,s），作为最优切分点。

### CART分类树生成
分类树用 *基尼指数* 选择最优特征，同时决定该特征的最优二值切分点。

![Image](https://github.com/user-attachments/assets/25c6a4a3-1d21-4421-8ae6-ddd47a9f6af4)

通俗的可以这么理解，基尼指数就是在样本集中随机抽出两个样本不同类别的概率。当样本集越不纯的时候，这个概率也就越大，即基尼指数也越大。这个规律与信息熵的相同，还是以刚才的只有两个取值的随机变量为例，我们这次纵坐标除了有信息熵外，再加上基尼指数。

![Image](https://i-blog.csdnimg.cn/blog_migrate/4a6a6352ebc0b5a3845bd241298a7bfa.png)

可以看出，基尼指数与信息熵虽然值不同，但是趋势一致。同样的，使用基尼指数来选择最优划分属性也是对比不同属性划分后基尼指数的差值，选择使样本集基尼指数减小最多的属性。

![Image](https://github.com/user-attachments/assets/ec3755fc-b624-42c8-8a39-e022432d1050)

**生成树的算法:**

![Image](https://github.com/user-attachments/assets/bc3e3aef-c790-4b7e-a64e-f7064a57d3a6)

### CART剪枝

直接上链接: https://zhuanlan.zhihu.com/p/76709712
